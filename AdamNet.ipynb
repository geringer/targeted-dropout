{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdamNet\n",
    "\n",
    "A closer look at targeted dropout\n",
    "\n",
    "### Let's install our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras) (5.1.1)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "absl-py==0.7.0\n",
      "astor==0.7.1\n",
      "attrs==18.2.0\n",
      "backcall==0.1.0\n",
      "bleach==3.1.0\n",
      "cycler==0.10.0\n",
      "decorator==4.3.2\n",
      "defusedxml==0.5.0\n",
      "entrypoints==0.3\n",
      "enum34==1.1.6\n",
      "gast==0.2.2\n",
      "grpcio==1.19.0\n",
      "h5py==2.9.0\n",
      "ipykernel==5.1.0\n",
      "ipython==7.3.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "jedi==0.13.3\n",
      "Jinja2==2.10\n",
      "jsonschema==3.0.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.4\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.4.0\n",
      "jupyter-http-over-ws==0.0.3\n",
      "Keras==2.2.4\n",
      "Keras-Applications==1.0.7\n",
      "Keras-Preprocessing==1.0.9\n",
      "kiwisolver==1.0.1\n",
      "Markdown==3.0.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.0.2\n",
      "mistune==0.8.4\n",
      "mock==2.0.0\n",
      "nbconvert==5.4.1\n",
      "nbformat==4.4.0\n",
      "notebook==5.7.4\n",
      "numpy==1.16.2\n",
      "pandocfilters==1.4.2\n",
      "parso==0.3.4\n",
      "pbr==5.1.2\n",
      "pexpect==4.6.0\n",
      "pickleshare==0.7.5\n",
      "prometheus-client==0.6.0\n",
      "prompt-toolkit==2.0.9\n",
      "protobuf==3.6.1\n",
      "ptyprocess==0.6.0\n",
      "pycurl==7.43.0\n",
      "Pygments==2.3.1\n",
      "pygobject==3.20.0\n",
      "pyparsing==2.3.1\n",
      "pyrsistent==0.14.11\n",
      "python-apt==1.1.0b1+ubuntu0.16.4.2\n",
      "python-dateutil==2.8.0\n",
      "PyYAML==5.1.1\n",
      "pyzmq==18.0.0\n",
      "qtconsole==4.4.3\n",
      "scipy==1.3.0\n",
      "Send2Trash==1.5.0\n",
      "six==1.12.0\n",
      "tensorboard==1.13.0\n",
      "tensorflow-estimator==1.13.0\n",
      "tensorflow-gpu==1.13.1\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.4.2\n",
      "tornado==5.1.1\n",
      "traitlets==4.3.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will train our neural network\n",
    "\n",
    "Please note, in the process of training this, I took some time to play around and push the hyperparams around for fun trying to increase test accuracy to over 90%. But, if this does not sound fun to you, I have attached the model weights, and you can [skip ahead](#pruning) to that part of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.4606 - acc: 0.8321\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.3488 - acc: 0.8714\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3105 - acc: 0.8853\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2855 - acc: 0.8941\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.2666 - acc: 0.8996\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.2513 - acc: 0.9055\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2341 - acc: 0.9112\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.2235 - acc: 0.9154\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2116 - acc: 0.9197\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2005 - acc: 0.9232\n",
      "10000/10000 [==============================] - 2s 172us/step\n",
      "Test Accuracy =  0.8905\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dense(200, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "])\n",
    "    \n",
    "optimizer = Adam(lr = 0.0003)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, batch_size = 32, epochs = 10)\n",
    "\n",
    "print(\"Test Accuracy = \", model.evaluate(test_images, test_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pruning'></a>\n",
    "# Pruning AdamNet\n",
    "\n",
    "Skip to here if you do not want to recook my network.\n",
    "\n",
    "### Start by loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we need to construct our weight and unit pruning tools.\n",
    "\n",
    "### Weight Pruning\n",
    "\n",
    "I found this on [some blog somewhere](https://for.ai/blog/targeted-dropout/), and obfuscated the original author by changing some formating. \n",
    "\n",
    "Side note, the code from the blog was missing some things (where does w come from?), and since tensorflow is nice, but unessecary, I decided to replicate with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def prune_weights(weights, k, targeted_portion):\n",
    "    \n",
    "    weights_shape = weights.shape\n",
    "    weights_total = weights_shape[0] * weights_shape[1]\n",
    "    \n",
    "    w = np.reshape(weights, [weights_total, 1])\n",
    "    importance = np.absolute(w)\n",
    "    idx = targeted_portion # * w[0] #this is broken\n",
    "    \n",
    "    importance_threshold = np.sort(importance, axis=0)[idx]\n",
    "    \n",
    "    unimportance_mask = importance < importance_threshold[None, :] \n",
    "    \n",
    "#     lessthan_mask = np.random.uniform( w.shape ) < k #is this normalize wrt the weights [0:1]?\n",
    "#     print(lessthan_mask)\n",
    "#     dropout_mask = np.all(lessthan_mask, unimportance_mask)\n",
    "    \n",
    "#     weights = (1. - dropout_mask) * w\n",
    "#     weights = np.reshape(w, weights_shape)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# weights = loaded_model.layers[2].get_weights()\n",
    "\n",
    "# prune_weights(weights[0], .8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Pruning\n",
    "\n",
    "Architecture largely inspired by the authors of the previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_units(weights, k, targeted_portion):\n",
    "    \n",
    "    \"\"\"\n",
    "    organize into units... how?\n",
    "    \n",
    "    grapple their incoming connections\n",
    "    \n",
    "    find the L1 norm of those connections\n",
    "    \n",
    "    rank the units by the L1 connections\n",
    "    \n",
    "    select the lower k% of those units\n",
    "        change their inputs or outputs to zero\n",
    "        \n",
    "    return the new weight matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     weights_shape = weights.shape\n",
    "#     w = tf.reshape(weights, [-1,weights[-1]])\n",
    "    \n",
    "#     importance = tf.abs(w)\n",
    "#     idx = tf.to_int32(targeted_portion * tf.to_float(tf.shape(w)[0]))\n",
    "    \n",
    "#     importance_threshold = tf.contrib.framework.sort(importance, axis=0)[idx]\n",
    "#     unimportance_mask = importance < importance_threshold[None, :]\n",
    "  \n",
    "#     dropout_mask = tf.to_float(tf.logical_and(tf.random_uniform(tf.shape(weights)) < \n",
    "#                                               k, unimportance_mask))\n",
    "    \n",
    "#     weights = (1. - dropout_mask) * weights\n",
    "#     weights = tf.reshape(weights, weights_shape)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the Original Model\n",
    "\n",
    "Now that we have our methods that return pruned weight matrixes, we must implement them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph and Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "Find a guide to manipulating weight matrices\n",
    "\n",
    "Edit dockerfile to execute to this jupyter notebook\n",
    "\n",
    "Build repository + README.md\n",
    "\n",
    "edit var names\n",
    "\n",
    "maybe add an exploration into which type of hyper params cause more or less zeroing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules\n",
    "\n",
    "1. Please use TensorFlow (it’s what we use for all of our projects). Consider using colab for access to free GPUs/TPUs.\n",
    "\n",
    "2. You may use frameworks or libraries as you see fit. If you borrow code please include proper attribution and have​ a ​clear​ ​separation​ ​between​ ​the​ ​code​ ​you borrowed​ ​and​ ​the​ ​code​ ​you​ ​wrote​ ​yourself​.\n",
    "\n",
    "3. You should keep​ ​your​ ​code​ ​simple​ ​and​ ​focus​ ​on​ ​readability​ of your code. Include any instructions for running and reading your code in a README file. We​ ​value​ ​thoughtfully written ​ ​clean,​ ​and​ ​communicative​ ​code​ ​so other​ ​contributors​ ​can​ ​easily​ ​understand​ ​and​ ​build​ ​on​ ​top​ ​of​ ​it.\n",
    "\n",
    "4. You may skip any parts of the challenge if you get stuck or don’t have relevant experience. However, we encourage you to learn and demonstrate newly acquired skills. \n",
    "\n",
    "5. You should check your solution into GitHub​ or Colab and provide basic instructions on how to reproduce your results. \n",
    "\n",
    "6. You are free to spend as little or as much time as you want on this challenge.\n",
    "\n",
    "7. You are expected to learn something new after you complete the challenge :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPT\n",
    "\n",
    "[Here is the original google doc](https://docs.google.com/document/d/1cW-bP_7hw22Wi5nwWOcmMo7Pp9J04nwif8OiNqXRQ3o/edit)\n",
    "\n",
    "1. Read the ​Rules​! \n",
    "\n",
    "2. Install Tensorflow \n",
    "\n",
    "3. Construct a ReLU-activated neural network with four hidden layers with sizes [1000, 1000, 500, 200]. Note: you’ll have a fifth layer for your output logits, which you will have 10 of.\n",
    "\n",
    "4. Prune away (set to zero) the k% of weights using weight and unit pruning for k in [0, 25, 50, 60, 70, 80, 90, 95, 97, 99]. Remember not to prune the weights leading to the output logits.\n",
    "\n",
    "5. Create a table or plot showing the percent sparsity (number of weights in your network that are zero) versus percent accuracy with two curves (one for weight pruning and one for unit pruning).\n",
    "\n",
    "6. Make your code clean and readable. Add comments where needed. \n",
    "\n",
    "7. Analyze your results. What interesting insights did you find? Do the curves differ? Why do you think that is/isn’t? \n",
    "\n",
    "8. Do you have any hypotheses as to why we are able to delete so much of the network without hurting performance (this is an open research question)?\n",
    "\n",
    "9. Bonus: See if you can find a way to use your new-found sparsity to speed up the execution of your neural net! Hint: ctrl + f “sparse” in the TF docs, or use unit level sparsity (which deletes entire rows and columns from weight matrices). This can be tricky but is a worthwhile engineering lesson in the optimization of Tensorflow models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the categories to the labels\n",
    "predictions = np_utils.categorical_probas_to_classes(predictions)\n",
    "\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "\n",
    "#Writing data to the output\n",
    "out = numpy.column_stack((range(1, predictions.shape[0]+1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/keras-team/keras/issues/91#issuecomment-97583594\n",
    "\n",
    "import h5py\n",
    "\n",
    "def print_structure(weight_file_path):\n",
    "    f = h5py.File(weight_file_path)\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))\n",
    "\n",
    "        if len(f.items())==0:\n",
    "            return \n",
    "\n",
    "        for layer, g in f.items():\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items():\n",
    "                print(\"      {}: {}\".format(key, value))\n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                print(\"      {}: {}\".format(p_name, param.shape))\n",
    "    finally:\n",
    "        f.close()\n",
    "        \n",
    "print_structure(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
