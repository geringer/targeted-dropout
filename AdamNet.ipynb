{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdamNet\n",
    "\n",
    "A closer look at targeted dropout\n",
    "\n",
    "### Let's install our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 8.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib64/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/lib64/python3.6/site-packages (from keras) (1.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib64/python3.6/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/lib64/python3.6/site-packages (from keras) (1.13.3)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python3.6/site-packages (from keras) (3.12)\n",
      "Installing collected packages: keras\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/lib64/python3.6/site-packages/Keras-2.2.4.dist-info'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib64/python3.6/site-packages (1.13.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (1.13.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib64/python3.6/site-packages (from tensorflow) (1.20.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/lib64/python3.6/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib64/python3.6/site-packages (from tensorflow) (3.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib64/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib64/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib64/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib64/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (37.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "# !pip freeze\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will train our neural network\n",
    "\n",
    "Please note, in the process of training this, I took some time to play around and push the hyperparams around for fun trying to increase test accuracy to over 90%. But, if this does not sound fun to you, I have attached the model weights, and you can [skip ahead](#pruning) to that part of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.4606 - acc: 0.8321\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.3488 - acc: 0.8714\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3105 - acc: 0.8853\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2855 - acc: 0.8941\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.2666 - acc: 0.8996\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.2513 - acc: 0.9055\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2341 - acc: 0.9112\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.2235 - acc: 0.9154\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2116 - acc: 0.9197\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2005 - acc: 0.9232\n",
      "10000/10000 [==============================] - 2s 172us/step\n",
      "Test Accuracy =  0.8905\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dense(200, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "])\n",
    "    \n",
    "optimizer = Adam(lr = 0.0003)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, batch_size = 32, epochs = 10)\n",
    "\n",
    "print(\"Test Accuracy = \", model.evaluate(test_images, test_labels)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pruning'></a>\n",
    "# Pruning AdamNet\n",
    "\n",
    "Skip to here if you do not want to recook my network.\n",
    "\n",
    "### Start by loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we need to construct our weight and unit pruning tools.\n",
    "\n",
    "### Weight Pruning\n",
    "\n",
    "I found this on [some blog somewhere](https://for.ai/blog/targeted-dropout/), and obfuscated the original author by changing some formating ;)\n",
    "\n",
    "Side note, the code from the blog was missing some things (where does w come from?), and since tensorflow is nice, but unessecary, I decided to replicate with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Less_2:0\", shape=(1000,), dtype=bool)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-ab861cff387b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-ab861cff387b>\u001b[0m in \u001b[0;36mprune_weights\u001b[0;34m(weights, k, targeted_unit)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlessthan_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munimportance_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#WOOHOOO DEMORGANS LAW BABY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlessthan_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;31m#dropout_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def prune_weights(weights, k, targeted_unit):\n",
    "    \n",
    "    weights_shape = weights.shape\n",
    "    weights_total = weights_shape[0] * weights_shape[1]\n",
    "    \n",
    "    w = weights[:,targeted_unit - 1]\n",
    "    \n",
    "    importance = np.absolute(w)\n",
    "    importance_threshold = np.sort(importance, axis=0)\n",
    "    unimportance_mask = importance < importance_threshold \n",
    "    \n",
    "#     lessthan_mask = np.random.uniform( w.shape ) < k\n",
    "    lessthan_mask = tf.random_uniform(tf.shape(w)) < k\n",
    "    print(lessthan_mask)\n",
    "#                                                  \n",
    "    dropout_mask = np.all(np.random.uniform( w.shape ) < k, unimportance_mask) #WOOHOOO DEMORGANS LAW BABY\n",
    "    \n",
    "    w = (1. - lessthan_mask) * w #dropout_mask\n",
    "    \n",
    "    weights[:, targeted_unit - 1] = w\n",
    "    return weights\n",
    "\n",
    "\n",
    "weights = loaded_model.layers[3].get_weights()[0]\n",
    "\n",
    "new = prune_weights(weights, .8, 5)\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Unit Pruning\n",
    "\n",
    "Architecture largely inspired by the authors of the previous method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19, 30.1453), (160, 30.48764), (190, 30.670528), (197, 30.691174), (273, 30.759851), (238, 30.882511), (277, 30.885025), (165, 30.89934), (203, 30.919605), (178, 30.922749), (41, 30.965157), (481, 30.99667), (324, 31.006624), (341, 31.00729), (272, 31.017365), (212, 31.070967), (296, 31.076511), (499, 31.100332), (357, 31.112171), (93, 31.149559), (215, 31.159388), (302, 31.165188), (242, 31.18659), (344, 31.205662), (309, 31.224598), (245, 31.235931), (394, 31.252478), (437, 31.259018), (280, 31.306561), (198, 31.323797), (244, 31.331583), (107, 31.354841), (469, 31.375763), (94, 31.376011), (345, 31.384953), (379, 31.394802), (59, 31.423632), (480, 31.457111), (372, 31.471775), (434, 31.485882), (172, 31.48941), (260, 31.491617), (233, 31.509159), (102, 31.514992), (320, 31.527737), (58, 31.549093), (450, 31.557928), (322, 31.573526), (35, 31.57785), (6, 31.584248), (405, 31.584389), (335, 31.584576), (99, 31.585121), (412, 31.588703), (287, 31.611925), (447, 31.628237), (209, 31.64094), (402, 31.645885), (83, 31.656342), (133, 31.666756), (396, 31.673313), (489, 31.679394), (1, 31.68124), (364, 31.691536), (53, 31.722248), (220, 31.723734), (461, 31.724482), (132, 31.726192), (184, 31.730324), (401, 31.741856), (60, 31.752672), (138, 31.765079), (496, 31.766312), (72, 31.781542), (150, 31.784142), (338, 31.790518), (111, 31.79068), (67, 31.790981), (95, 31.793674), (193, 31.801298), (482, 31.801744), (248, 31.808386), (262, 31.819536), (286, 31.822357), (465, 31.835369), (122, 31.853039), (76, 31.853743), (69, 31.860893), (66, 31.87834), (194, 31.881207), (347, 31.885992), (123, 31.88974), (424, 31.890152), (48, 31.893082), (383, 31.893845), (98, 31.894848), (239, 31.910601), (485, 31.921513), (304, 31.921638), (13, 31.924217), (199, 31.924614), (183, 31.926914), (204, 31.927004), (177, 31.935379), (278, 31.947098), (343, 31.960556), (377, 31.972828), (216, 31.975758), (223, 31.976805), (24, 31.979443), (421, 31.981312), (232, 31.994274), (121, 32.000431), (153, 32.00069), (429, 32.009327), (261, 32.018578), (288, 32.022095), (346, 32.028484), (351, 32.028564), (291, 32.033642), (208, 32.059795), (219, 32.065598), (169, 32.066105), (175, 32.071533), (68, 32.078583), (458, 32.080379), (428, 32.083263), (430, 32.088882), (385, 32.096771), (254, 32.097412), (0, 32.107529), (36, 32.108597), (228, 32.109661), (11, 32.113194), (45, 32.11644), (241, 32.122375), (270, 32.125111), (407, 32.128918), (185, 32.134171), (246, 32.137672), (186, 32.142986), (39, 32.144093), (361, 32.146397), (425, 32.152542), (22, 32.153324), (144, 32.168068), (96, 32.171097), (230, 32.1726), (139, 32.173286), (439, 32.1735), (118, 32.180576), (471, 32.18251), (366, 32.198948), (300, 32.207096), (211, 32.207146), (108, 32.211655), (416, 32.211868), (114, 32.214394), (275, 32.220867), (363, 32.223137), (274, 32.225639), (47, 32.226871), (477, 32.227825), (201, 32.228745), (74, 32.229881), (10, 32.230774), (136, 32.241898), (321, 32.243317), (339, 32.252735), (226, 32.253933), (255, 32.257004), (473, 32.25853), (229, 32.26265), (307, 32.265083), (182, 32.268154), (40, 32.27314), (163, 32.275826), (164, 32.283012), (176, 32.297653), (414, 32.298714), (264, 32.298771), (179, 32.304199), (227, 32.307472), (325, 32.307842), (492, 32.319099), (2, 32.319691), (29, 32.3242), (222, 32.324398), (433, 32.325127), (50, 32.326416), (61, 32.330685), (462, 32.333351), (464, 32.334339), (454, 32.334663), (400, 32.347092), (157, 32.347672), (315, 32.348526), (5, 32.348747), (128, 32.34948), (181, 32.363289), (497, 32.366379), (217, 32.380669), (240, 32.381462), (73, 32.386177), (440, 32.391212), (318, 32.396252), (256, 32.401669), (33, 32.406563), (154, 32.412174), (478, 32.415642), (235, 32.416267), (253, 32.422203), (110, 32.422482), (126, 32.427151), (80, 32.427299), (289, 32.427902), (187, 32.428501), (57, 32.429516), (263, 32.429787), (151, 32.432667), (365, 32.437668), (367, 32.444248), (479, 32.44437), (411, 32.445251), (259, 32.450539), (26, 32.454529), (202, 32.45594), (77, 32.456249), (18, 32.45657), (449, 32.460526), (148, 32.463516), (221, 32.466736), (486, 32.468491), (17, 32.470234), (249, 32.482803), (155, 32.483383), (445, 32.484665), (393, 32.485603), (234, 32.486504), (56, 32.491829), (333, 32.492012), (331, 32.492722), (410, 32.501396), (330, 32.504665), (463, 32.511742), (368, 32.513084), (472, 32.517155), (311, 32.517925), (43, 32.533356), (101, 32.534275), (8, 32.535427), (7, 32.536179), (395, 32.5364), (295, 32.53693), (200, 32.537697), (387, 32.539948), (498, 32.541145), (438, 32.542355), (14, 32.543015), (384, 32.543457), (28, 32.556892), (362, 32.559597), (243, 32.56411), (100, 32.567116), (125, 32.571644), (206, 32.580109), (174, 32.584209), (408, 32.585617), (156, 32.589439), (382, 32.58947), (494, 32.58989), (44, 32.589993), (453, 32.595436), (225, 32.598503), (252, 32.599197), (54, 32.602821), (474, 32.605797), (250, 32.612202), (342, 32.619026), (55, 32.622559), (88, 32.6287), (490, 32.635406), (317, 32.636688), (313, 32.637756), (120, 32.641083), (432, 32.642597), (327, 32.648911), (166, 32.650684), (147, 32.651066), (388, 32.651306), (299, 32.651413), (70, 32.652512), (356, 32.65963), (113, 32.661362), (237, 32.665676), (285, 32.668976), (167, 32.669792), (348, 32.670403), (218, 32.673088), (231, 32.673088), (386, 32.681755), (334, 32.686588), (62, 32.690727), (426, 32.69289), (3, 32.69677), (444, 32.69738), (375, 32.699348), (323, 32.700325), (31, 32.703682), (493, 32.70591), (376, 32.705936), (104, 32.707733), (413, 32.710884), (460, 32.713928), (12, 32.715336), (146, 32.715492), (97, 32.718254), (168, 32.720459), (491, 32.725689), (350, 32.72596), (162, 32.729061), (15, 32.729702), (292, 32.730145), (25, 32.732162), (159, 32.736259), (298, 32.749214), (380, 32.750103), (495, 32.752319), (369, 32.752525), (112, 32.753662), (213, 32.754662), (91, 32.75473), (431, 32.763824), (398, 32.764824), (52, 32.765007), (392, 32.768333), (258, 32.773792), (103, 32.776283), (115, 32.781555), (86, 32.785725), (46, 32.790359), (374, 32.791672), (417, 32.801598), (483, 32.809078), (403, 32.810585), (326, 32.813728), (476, 32.813774), (30, 32.822906), (207, 32.823792), (189, 32.825081), (422, 32.832764), (170, 32.83802), (314, 32.838753), (337, 32.846741), (409, 32.857464), (51, 32.860527), (224, 32.860588), (79, 32.868374), (21, 32.86861), (180, 32.869045), (442, 32.873604), (205, 32.875877), (279, 32.888065), (371, 32.888481), (134, 32.897301), (328, 32.897842), (124, 32.898205), (415, 32.904602), (389, 32.908085), (191, 32.913658), (268, 32.916386), (196, 32.921555), (129, 32.923721), (188, 32.925659), (142, 32.933914), (370, 32.934067), (310, 32.934097), (358, 32.945866), (441, 32.956505), (443, 32.958973), (466, 32.961254), (306, 32.968208), (92, 32.969791), (284, 32.970573), (81, 32.971561), (135, 32.975307), (305, 32.9776), (173, 32.987526), (282, 32.988735), (210, 32.988983), (487, 32.992641), (448, 32.998619), (87, 33.000546), (451, 33.010025), (37, 33.011177), (106, 33.026066), (119, 33.029259), (127, 33.0322), (23, 33.037689), (32, 33.03941), (109, 33.042454), (130, 33.04377), (158, 33.047836), (105, 33.050613), (116, 33.054138), (423, 33.055809), (65, 33.056122), (49, 33.057167), (354, 33.063457), (435, 33.073341), (143, 33.0746), (247, 33.089287), (391, 33.107269), (418, 33.111687), (332, 33.113197), (353, 33.12075), (9, 33.122139), (265, 33.124088), (475, 33.138382), (355, 33.159527), (397, 33.169304), (137, 33.175247), (269, 33.181633), (161, 33.1828), (266, 33.189705), (257, 33.193699), (359, 33.199978), (16, 33.215801), (319, 33.216736), (293, 33.229614), (251, 33.232723), (297, 33.234489), (85, 33.234642), (381, 33.236748), (455, 33.236843), (420, 33.250473), (63, 33.261044), (214, 33.272453), (373, 33.280334), (390, 33.282753), (149, 33.28727), (131, 33.288162), (399, 33.289848), (290, 33.29493), (20, 33.298813), (117, 33.30896), (452, 33.311752), (436, 33.313255), (468, 33.314232), (195, 33.320023), (360, 33.326584), (303, 33.331139), (171, 33.334335), (4, 33.344376), (34, 33.345329), (64, 33.356728), (276, 33.356873), (406, 33.357895), (283, 33.358757), (82, 33.362309), (89, 33.363873), (427, 33.371395), (42, 33.383617), (84, 33.38987), (459, 33.397385), (78, 33.399109), (38, 33.4151), (152, 33.434395), (456, 33.436069), (71, 33.437538), (484, 33.443607), (467, 33.484489), (470, 33.49147), (90, 33.500458), (457, 33.511864), (316, 33.516678), (140, 33.529915), (308, 33.538837), (419, 33.561516), (141, 33.561798), (267, 33.574913), (329, 33.580185), (340, 33.599915), (281, 33.60593), (75, 33.608566), (378, 33.703033), (336, 33.703438), (236, 33.711777), (145, 33.712852), (488, 33.724979), (404, 33.734669), (294, 33.737923), (271, 33.792519), (349, 34.091667), (301, 34.128059), (352, 34.175674), (27, 34.228363), (312, 34.232391), (192, 34.274189), (446, 34.430641)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0358753 ,  0.00203734, -0.03108707, ...,  0.0472178 ,\n",
       "         0.05643078, -0.02678137],\n",
       "       [ 0.0421016 ,  0.05543165, -0.03793364, ..., -0.05707404,\n",
       "         0.04389518,  0.04346066],\n",
       "       [-0.04796297,  0.03422289, -0.01008656, ...,  0.01951565,\n",
       "         0.00379465,  0.03773079],\n",
       "       ..., \n",
       "       [ 0.01552797, -0.03858643,  0.01705509, ...,  0.05192143,\n",
       "        -0.0167532 , -0.00647251],\n",
       "       [-0.03163313,  0.01798149, -0.05768332, ..., -0.00999958,\n",
       "        -0.02496116,  0.03626469],\n",
       "       [ 0.04924151, -0.01077353,  0.01674547, ..., -0.06326522,\n",
       "        -0.0086967 , -0.02718662]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prune_units(weights, k):\n",
    "    \n",
    "    weights_shape = weights.shape\n",
    "    final_weights = np.zeros(weights_shape)\n",
    "    \n",
    "    num_cols = weights_shape[1]\n",
    "    \n",
    "    d = {} #nd init 2(x)num_cols\n",
    "    \n",
    "    for i in range(num_cols): #nditter\n",
    "        \n",
    "        column = weights[:,i]\n",
    "        \n",
    "        l1 = np.linalg.norm(column, ord=1)\n",
    "        d[i] = l1\n",
    "        \n",
    "    np.percentile(d , k) #to take the kth percentile\n",
    "    \n",
    "    #create a mask\n",
    "    \n",
    "    #if mask then populate keyth column or final_weights with original weights column\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    organize into units... how?\n",
    "    \n",
    "    grapple their incoming connections\n",
    "    \n",
    "    find the L1 norm of those connections\n",
    "    \n",
    "    rank the units by the L1 connections\n",
    "    \n",
    "    select the lower k% of those units\n",
    "        change their inputs to zero\n",
    "        \n",
    "    return the new weight matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     weights_shape = weights.shape\n",
    "#     w = tf.reshape(weights, [-1,weights[-1]])\n",
    "    \n",
    "#     importance = tf.abs(w)\n",
    "#     idx = tf.to_int32(targeted_portion * tf.to_float(tf.shape(w)[0]))\n",
    "    \n",
    "#     importance_threshold = tf.contrib.framework.sort(importance, axis=0)[idx]\n",
    "#     unimportance_mask = importance < importance_threshold[None, :]\n",
    "  \n",
    "#     dropout_mask = tf.to_float(tf.logical_and(tf.random_uniform(tf.shape(weights)) < \n",
    "#                                               k, unimportance_mask))\n",
    "    \n",
    "#     weights = (1. - dropout_mask) * weights\n",
    "#     weights = tf.reshape(weights, weights_shape)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "weights = loaded_model.layers[3].get_weights()[0]\n",
    "\n",
    "prune_units(weights , 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the Original Model\n",
    "\n",
    "Now that we have our methods that return pruned weight matrixes, we must implement them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph and Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " percents = [.0, .25, .50, .60, .70, .80, .90, .95, .97, .99]\n",
    "\n",
    "for k in percents:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "Find a guide to manipulating weight matrices\n",
    "\n",
    "Edit dockerfile to execute to this jupyter notebook\n",
    "\n",
    "Build repository + README.md\n",
    "\n",
    "edit var names\n",
    "\n",
    "maybe add an exploration into which type of hyper params cause more or less zeroing\n",
    "\n",
    "comments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules\n",
    "\n",
    "1. Please use TensorFlow (it’s what we use for all of our projects). Consider using colab for access to free GPUs/TPUs.\n",
    "\n",
    "2. You may use frameworks or libraries as you see fit. If you borrow code please include proper attribution and have​ a ​clear​ ​separation​ ​between​ ​the​ ​code​ ​you borrowed​ ​and​ ​the​ ​code​ ​you​ ​wrote​ ​yourself​.\n",
    "\n",
    "3. You should keep​ ​your​ ​code​ ​simple​ ​and​ ​focus​ ​on​ ​readability​ of your code. Include any instructions for running and reading your code in a README file. We​ ​value​ ​thoughtfully written ​ ​clean,​ ​and​ ​communicative​ ​code​ ​so other​ ​contributors​ ​can​ ​easily​ ​understand​ ​and​ ​build​ ​on​ ​top​ ​of​ ​it.\n",
    "\n",
    "4. You may skip any parts of the challenge if you get stuck or don’t have relevant experience. However, we encourage you to learn and demonstrate newly acquired skills. \n",
    "\n",
    "5. You should check your solution into GitHub​ or Colab and provide basic instructions on how to reproduce your results. \n",
    "\n",
    "6. You are free to spend as little or as much time as you want on this challenge.\n",
    "\n",
    "7. You are expected to learn something new after you complete the challenge :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPT\n",
    "\n",
    "[Here is the original google doc](https://docs.google.com/document/d/1cW-bP_7hw22Wi5nwWOcmMo7Pp9J04nwif8OiNqXRQ3o/edit)\n",
    "\n",
    "1. Read the ​Rules​! \n",
    "\n",
    "2. Install Tensorflow \n",
    "\n",
    "3. Construct a ReLU-activated neural network with four hidden layers with sizes [1000, 1000, 500, 200]. Note: you’ll have a fifth layer for your output logits, which you will have 10 of.\n",
    "\n",
    "4. Prune away (set to zero) the k% of weights using weight and unit pruning for k in [0, 25, 50, 60, 70, 80, 90, 95, 97, 99]. Remember not to prune the weights leading to the output logits.\n",
    "\n",
    "5. Create a table or plot showing the percent sparsity (number of weights in your network that are zero) versus percent accuracy with two curves (one for weight pruning and one for unit pruning).\n",
    "\n",
    "6. Make your code clean and readable. Add comments where needed. \n",
    "\n",
    "7. Analyze your results. What interesting insights did you find? Do the curves differ? Why do you think that is/isn’t? \n",
    "\n",
    "8. Do you have any hypotheses as to why we are able to delete so much of the network without hurting performance (this is an open research question)?\n",
    "\n",
    "9. Bonus: See if you can find a way to use your new-found sparsity to speed up the execution of your neural net! Hint: ctrl + f “sparse” in the TF docs, or use unit level sparsity (which deletes entire rows and columns from weight matrices). This can be tricky but is a worthwhile engineering lesson in the optimization of Tensorflow models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "#### Interesting insights found, differing curves, potential reasons?\n",
    "\n",
    "\n",
    "#### Potential reasons for successfully deleting massive amounts of the network\n",
    "- Relu is mostly 0\n",
    "- lack of training dropout encourages subnetwork dependance allowing for\n",
    "- lottery ticket hypothesis\n",
    "\n",
    "#### How much faster is this? %%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the categories to the labels\n",
    "predictions = np_utils.categorical_probas_to_classes(predictions)\n",
    "\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "\n",
    "#Writing data to the output\n",
    "out = numpy.column_stack((range(1, predictions.shape[0]+1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/keras-team/keras/issues/91#issuecomment-97583594\n",
    "\n",
    "import h5py\n",
    "\n",
    "def print_structure(weight_file_path):\n",
    "    f = h5py.File(weight_file_path)\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))\n",
    "\n",
    "        if len(f.items())==0:\n",
    "            return \n",
    "\n",
    "        for layer, g in f.items():\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items():\n",
    "                print(\"      {}: {}\".format(key, value))\n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                print(\"      {}: {}\".format(p_name, param.shape))\n",
    "    finally:\n",
    "        f.close()\n",
    "        \n",
    "print_structure(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
